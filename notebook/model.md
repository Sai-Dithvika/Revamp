
## AI-Powered Learning Assistant with GPT-3.5 and Hugging Face

## Overview
The AI-Powered Learning Assistant integrates cutting-edge natural language processing (NLP) technologies, including GPT-3.5 from OpenAI and Hugging Face models, to revolutionize educational support. It offers personalized quizzes generated by GPT-3.5 based on learners' performance and real-time doubt clarification using a chatbot powered by Hugging Face's GPT-3.5.

## Intel One API & GPT-3.5
This project leverages the power of Intel One API for deep learning computations and integrates GPT-3.5 from Open!AI and Hugging Face models for advanced natural language understanding, enabling personalized learning experiences and real-time assistance.

## Overview of chatbot

<div>
  <img src="https://github.com/Sai-Dithvika/Vashisht-Hackathon/assets/118179484/29910c7c-981a-4c34-96aa-eed80cf02584" width="200px" style="margin-right: 180px;"/> 
  
  <img src="https://github.com/Sai-Dithvika/Vashisht-Hackathon/assets/118179484/9ecd4f87-f629-4bd6-9283-f80fbec3f329" width="200px" style="margin-right: 40px;"/>
</div>

## Quiz finetuned Part


```
{
  "questions": [
    {
      "options": {
        "a": "Children working in safe environments",
        "b": "Children working within legal limits",
        "c": "Children engaging in work that is harmful to their health and development",
        "d": "Children volunteering their time to gain experience"
      },
      "question": "What is the definition of child labour?"
    },
    {
      "options": {
        "a": "It only affects a small number of children in developing countries",
        "b": "It is a rare occurrence in modern societies",
        "c": "It impacts millions of children around the world",
        "d": "It is a problem of the past"
      },
      "question": "How prevalent is child labour globally?"
    },
    {
      "options": {
        "a": "Working in family businesses",
        "b": "Domestic work",
        "c": "Attending school regularly",
        "d": "Playing with friends"
      },
      "question": "Identify at least two forms of child labour that exist."
    }
  ]
} ``` 
```
## This is how your ENV file Should be for DB and Openai
```
# Environment variables declared in this file are automatically made available to Prisma.
# See the documentation for more detail: https://pris.ly/d/prisma-schema#accessing-environment-variables-from-the-schema

# Prisma supports the native connection string format for PostgreSQL, MySQL, SQLite, SQL Server, MongoDB, and CockroachDB.
# See the documentation for all the connection string options: https://pris.ly/d/connection-strings

DATABASE_URL: ""
DB_USER: ""
DB_HOST: ""
DB_NAME: "postgres"
DB_PASS: ""
OPEN_API_KEY: ''
OPEN_API_ORG: '' 
PORT: 6969
EMAIL: ""
PASSWORD: ""

```
## This one is optional if you are using docker 

## Chatbot Bulding Overview with Hugginface
- How to use

You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we set a seed for reproducibility:

```
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("Hello, I'm a language model,", max_length=30, num_return_sequences=5)

[{'generated_text': "Hello, I'm a language model, a language for thinking, a language for expressing thoughts."},
 {'generated_text': "Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don"},
 {'generated_text': "Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help"},
 {'generated_text': "Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly"},
 {'generated_text': 'Hello, I\'m a language model, not a language model"\n\nThe concept of "no-tricks" comes in handy later with new'}]
```
## Here is how to use this model to get the features of a given text in PyTorch:

```
from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

# And in tensorflow
```
from transformers import GPT2Tokenizer, TFGPT2Model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = TFGPT2Model.from_pretrained('gpt2')
text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
```
